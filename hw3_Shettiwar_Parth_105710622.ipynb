{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkCMihw9gbNx"
      },
      "source": [
        "#  CS 247 : Advanced Data Mining Learning\n",
        "## Homework 3 - Solution\n",
        "\n",
        "### Due: 11:59 pm 02/06\n",
        "\n",
        "##### Please read the Homework Guidance carefully and make sure you fulfill all the requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTzA8QXLgbNz"
      },
      "source": [
        "## Problem 1: Multinomial Mixture Model (40 pts)\n",
        "\n",
        "Consider the __Multinomial Mixture Model__ on slide 05-TopicModels P10-P12. let $\\mathbf{x}_d=(x_{d1}, x_{d2}, \\cdots, x_{dN})$ be the bag-of-words representation of document $d$. Let $\\boldsymbol{\\pi}$ and $\\boldsymbol{\\beta}$ be the learned parameters. We know that the cluster label $z_d\\sim Categorical(\\boldsymbol{\\pi})$ and $\\mathbf{x}_d\\sim multinomial(\\boldsymbol{\\beta}_z)$. \n",
        "\n",
        "Please derive the expression of $P(z_d=k|\\mathbf{x}_d;\\boldsymbol{\\beta},\\boldsymbol{\\pi})$. The expression should only contain $\\boldsymbol{\\pi}$ and $\\boldsymbol{\\beta}$. \n",
        "\n",
        "Please include neccessary intermediate steps instead of simply write down the final results.\n",
        "\n",
        "Hint: $P(z_d=k|\\mathbf{x}_d;\\boldsymbol{\\beta},\\boldsymbol{\\pi})=\\frac{P(\\mathbf{x}_d|z_d=k;\\boldsymbol{\\beta},\\boldsymbol{\\pi})P(z_d=k|\\boldsymbol{\\beta},\\boldsymbol{\\pi})}{P(\\mathbf{x}_d|\\boldsymbol{\\beta},\\boldsymbol{\\pi})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEkB37XXgbNz"
      },
      "source": [
        "#### Write Your answer here:\n",
        "![pic](https://drive.google.com/uc?export=view&id=1hPFJHTMk7bCfIhlQqAGMMYvzPTcUr9Tn)\n",
        "\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWhdGIbUgbN1"
      },
      "source": [
        "## Problem 2: Probabilistic Latent Semantic Analysis （40 pts)\n",
        "\n",
        "In this problem, you will be given a toy dataset. And you are going to compute the E step and M step for EM algorithm of pLSA (slides 05 TopicModels P25) by hand. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have two documents and two topics. The vocabulary only contains 4 words: $\\{1:A, \\ 2:B,\\  3:C,\\  4:D\\}$. The bag-of-words representation of document $d_1$ is $(4,3,2,1)$, which means $d_1$ has 4 $A$, 3 $B$, 2 $C$, and 1 $D$. And the bag-of-words representation of $d_2$ is $(2,2,3,1)$.\n",
        "\n",
        "Let $P(z_1=1|d_1)=\\theta_{11}$, $P(z_1=2|d_1)=\\theta_{12}$, $P(z_1=1|d_2)=\\theta_{21}$, $P(z_1=2|d_2)=\\theta_{22}$. Let $P(w|z,d)=\\beta_{zw}$.\n",
        "\n",
        "Assume we initialize the parameters using the following values: $\\theta_{11}^{(0)}=0.3$, $\\theta_{21}^{(0)}=0.4$. Let $\\boldsymbol{\\beta}_1^{(0)}=(1,0,0,0)$, $\\boldsymbol{\\beta}_2^{(0)}=(0,0.4,0.3,0.3)$."
      ],
      "metadata": {
        "id": "ubUbMpivnriU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 E-step (20 pts)\n"
      ],
      "metadata": {
        "id": "MLeNmuRKnnEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please compute $P(z=1|w,d_1)$ for all the words using the initialized values given above."
      ],
      "metadata": {
        "id": "wcPDlCwfq2DX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your Answer Here\n",
        "\n",
        "![pic](https://drive.google.com/uc?export=view&id=1IGIUEbFzA0TZ57-7TU7PMXDvLZaErEnu)\n",
        "![pic](https://drive.google.com/uc?export=view&id=1MgmpBhBCaF82YKCwmkGklMDxG20H4Uc-)\n",
        "\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->"
      ],
      "metadata": {
        "id": "BALkPUYwrGB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 M-step (20 pts)\n",
        "\n",
        "Let $P(z=1|A,d_2)=1$,\n",
        "$P(z=1|B,d_2)=0$,\n",
        "$P(z=1|C,d_2)=0$,\n",
        "$P(z=1|D,d_2)=0$.\n",
        "Please use your results for the last question to do the M-step to compute the new values of $\\beta_{11}$, $\\beta_{12}$, $\\theta_{11}$ and $\\theta_{12}$."
      ],
      "metadata": {
        "id": "aXoQcsPpt5dW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your Answer Here\n",
        "\n",
        "![pic](https://drive.google.com/uc?export=view&id=1MgmpBhBCaF82YKCwmkGklMDxG20H4Uc-)\n",
        "\n",
        "![pic](https://drive.google.com/uc?export=view&id=1cNisWmgbl6DJ3muozLjKQQQ0Ym-4ozNZ)\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->\n",
        "\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->"
      ],
      "metadata": {
        "id": "fs-Bq1r5unlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3 Latent Dirichlet Allocation (20 pts)\n",
        "\n",
        "Consider the Latent Dirichlet Allocation (LDA) model introduced in slide 05-TopicModels P31-52. In this problem, you are going to learn how to apply the LDA model implemented in sklearn to a real-world dataset. You are going to cluster the training datasets and output the top words in each cluster. Then you are going to get the soft clustering result for a new document. (You only need to learn how to use the sklearn LDA model)\n",
        "\n",
        "Please complete the code to train the LDA model and use LDA model to cluster the new document X_test.\n",
        "\n",
        "\n",
        "__Please set the number of topics to 4.__\n",
        "\n",
        "__Please set the random_state for LDA to 0.__\n",
        "\n",
        "__Please keep the rest of the parameters to default__\n",
        "\n",
        "__Hint:__\n",
        "\n",
        "You can refer to\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
        "to get familiar with sklearn LatendDirichletAllocation"
      ],
      "metadata": {
        "id": "6EVBDr5hzk_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sLuSTdmpgbN6"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',  'comp.graphics', 'sci.med']\n",
        "dataset = fetch_20newsgroups(shuffle=True, categories=categories, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq63MuH9gbN6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "no_features = 1000\n",
        "\n",
        "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
        "X = tf_vectorizer.fit_transform(documents)\n",
        "X_train = X[:-1]\n",
        "X_test = X[-1]\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 10"
      ],
      "metadata": {
        "id": "rmuYEXxG_9gm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "\"\"\"\n",
        "  TODO: implement the code to train LDA model using X_train and\n",
        "  use your trained model to cluster the new document X_test and print\n",
        "  the soft clustering probability.\n",
        "  The display_topics function will output the top words of each topic\n",
        "  Please use the variable name \"lda\" to name your model\n",
        "  Please set the number of topics to 4\n",
        "  Please set the random_state to 0.\n",
        "\"\"\"\n",
        "lda = LatentDirichletAllocation(n_components=4,random_state=0)\n",
        "lda.fit(X_train)\n",
        "print(\"soft clustering probability = \", lda.transform(X_test))\n",
        "display_topics(lda, tf_feature_names, no_top_words)"
      ],
      "metadata": {
        "id": "ICOZtMFN_l4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d287f9-f671-493a-f970-dd0b0e87761a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soft clustering probability =  [[0.0226685  0.00157616 0.42924712 0.54650823]]\n",
            "Topic 0:\n",
            "image graphics file jpeg use files software program images available\n",
            "Topic 1:\n",
            "god jesus people does believe bible church say faith christian\n",
            "Topic 2:\n",
            "edu com health research medical university 1993 patients information use\n",
            "Topic 3:\n",
            "don think people just like know time ve good really\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Problem Add-1 Smoothing and Dirichlet Prior (10 pts)\n",
        "\n",
        "In this problem, you are going to explore the relationship between add-1 smoothing and Dirichlet Prior. You can refer to slides P32-38 for the details of Dirichlet Prior.\n",
        "\n",
        "Assume $\\mathbf{x}\\in\\mathbf{Z}_+^N$ is the bag-of-words representation of a document, where $N$ is the total number of words in the vacabulary and $x_n$ denotes the number of occurences of word $n$ in the document. Assume $\\mathbf{x}$ follows multinomial distribution, $\\mathbf{x}\\sim Multinomial(\\boldsymbol{\\beta})$, i.e., $P(\\mathbf{x}|\\boldsymbol{\\beta})∝\\beta_1^{x_1}\\beta_2^{x_2}\\cdots\\beta_N^{x_N}$, $\\beta_n\\ge0$, $\\forall n$, and $\\sum_n\\beta_n=1$."
      ],
      "metadata": {
        "id": "SyQ9pgeqhXpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 (2 pts)\n",
        "\n",
        "Please derive the maximum likelihood estimation of $\\boldsymbol{\\beta}$ given $\\boldsymbol{x}$."
      ],
      "metadata": {
        "id": "DCF28g8Yvzk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your Answer Here\n",
        "![pic](https://drive.google.com/uc?export=view&id=1jvBQjVG-EYKjdCB11Yue-24M4xF1QURB)\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->"
      ],
      "metadata": {
        "id": "QV52aKHHxYky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 (2 pts)\n",
        "Some words may have 0 occurences, and we want to use add-1 smoothing when computing $\\beta$. Please write down the add-1 smoothing expression for $\\beta_n$."
      ],
      "metadata": {
        "id": "q642rQzQyGBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your Answer Here\n",
        "![pic](https://drive.google.com/uc?export=view&id=1fEmKDi2h5Ks46FvlOzt19xRVJF6LuBz0)\n",
        "\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->\n",
        "\n"
      ],
      "metadata": {
        "id": "K9jJNBEWyUGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3 (6 pts)\n",
        "Prove that add-1 smoothing is equivelant to assigning a Dirichlet Prior to $\\boldsymbol{\\beta}$, i.e., $\\boldsymbol{\\beta}\\sim Dirichlet(\\boldsymbol{\\alpha})$, where $\\alpha_n=1$, $\\forall n=1,2,\\cdots,N$.\n",
        "\n",
        "__Hint :__\n",
        "\n",
        "1. Consider $P(\\boldsymbol{\\beta}|\\mathbf{x},\\boldsymbol{\\alpha})$.\n",
        "2. If $\\boldsymbol{\\beta}\\sim Dirichlet(\\boldsymbol{\\alpha})$, then $E(\\beta_n)=\\frac{\\alpha_n}{\\sum_{n'}\\alpha_{n'}}$. "
      ],
      "metadata": {
        "id": "Hh1dfjqGybiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your Answer Here\n",
        "![pic](https://drive.google.com/uc?export=view&id=1fEmKDi2h5Ks46FvlOzt19xRVJF6LuBz0)\n",
        "![pic](https://drive.google.com/uc?export=view&id=1CaCEhPBPqxDvQGqqzdTkUvWYkLEDEVl4)\n",
        "\n",
        "<!-- https://drive.google.com/file/d//view?usp=sharing -->"
      ],
      "metadata": {
        "id": "a4NPNMSP2uQW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}