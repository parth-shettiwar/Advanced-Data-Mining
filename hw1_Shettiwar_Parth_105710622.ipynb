{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_lSsVJ9CYi3"
      },
      "source": [
        "#  CS 247 : Advanced Data Mining Learning\n",
        "## Homework 1\n",
        "\n",
        "### Due: 11:59 pm 01/14\n",
        "\n",
        "##### Please read the Homework Guidance (uploaded to Canvas) carefully and make sure you fulfill all the requirements.\n",
        "\n",
        "__Name__: Parth Shettiwar\n",
        "\n",
        "__UID__: 105710622"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kvI22PICYi5"
      },
      "source": [
        "## Problem 1: Multinomial Naive Bayes (50 pts)\n",
        "\n",
        "Consider the Multinomial Naive Bayes introduced in [lecture 02 - Probabilistic Classifiers](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf) (please refer to page 10-23). In this problem, you are going to understand the derivations of Multinomial Naive Bayes and apply it on a real-world classification task.\n",
        "\n",
        "### Part 1: Parameter Derivation (10 pts)\n",
        "\n",
        "Show the derivation process to obtain the solution of the MLE estimator $\\pi$. Please use the same notations as in the lecture slides.\n",
        "\n",
        "__Hint__: The solution of $\\pi$ is given on slide P23.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfMLr4IsCYi6"
      },
      "source": [
        "![pic](https://drive.google.com/uc?export=view&id=1eDMjB6P1Qa9HBWZXXYwBOYWOje6ryHSQ)\n",
        "![pic](https://drive.google.com/uc?export=view&id=1C6t3LEXQjmqoF45AuHSVKGQsJp_Sf1At)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1K6RDraCYi7"
      },
      "source": [
        "### Part 2: Text Classification (40 pts)\n",
        "\n",
        "In this part, you are going to apply the multinomial naive bayes method learned in the lecture on a real-world sentiment classification dataset. \n",
        "\n",
        "We've provided a general framework for you, please fill all the ''TODO'' slots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhoS3P8nCYi7"
      },
      "source": [
        "#### Part 2.1: Sklearn Implementation (5 pts)\n",
        "\n",
        "In this part, you are going to work on the text classification task using the multinomial naive bayes function __MultinomialNB__ implemented in the sklearn library. We've provided the data processing parts, please implememt the code for training and testing, and get the probability result using ***pred_proba*** and ***pred_log_proba***.\n",
        "\n",
        "\n",
        "__Hint__: \n",
        "\n",
        "1. You can refer to https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html to get familiar with sklearn MultinomialNB function.\n",
        "\n",
        "2. To get the predicted results, instead of directly using the ***predict*** function in MultinomialNB, please use <u>***np.argmax(..., axis=1)***</u>, and the ... part should be the probability result obtained from ***pred_proba*** or ***pred_log_proba***.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CEYbSxOdCYi7"
      },
      "outputs": [],
      "source": [
        "# load dataset, split train and test \n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',  'comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "arjdl6m_CYi8"
      },
      "outputs": [],
      "source": [
        "# data processing, turn the loaded data into array\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer().fit(twenty_train['data'] + twenty_test['data']) \n",
        "X_train_feature = count_vect.transform(twenty_train['data']).toarray()\n",
        "X_test_feature  = count_vect.transform(twenty_test['data']).toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zdqZW68NCYi8"
      },
      "outputs": [],
      "source": [
        "# train and test with MultinomialNB\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "    TO DO: \n",
        "    Please implement train and test using sklearn MultinomialNB.\n",
        "    You are expected to get the probability result using \"pred_proba\" and \"pred_log_proba\".\n",
        "'''\n",
        "\n",
        "def train(trn):\n",
        "  \n",
        "  trn.fit(X_train_feature, twenty_train[\"target\"])\n",
        "  return trn\n",
        "\n",
        "def test(trn):\n",
        "\n",
        "  prob_results_wo_log = trn.predict_proba(X_test_feature)\n",
        "  prob_results = trn.predict_log_proba(X_test_feature)\n",
        "\n",
        "  return prob_results, prob_results_wo_log\n",
        "\n",
        "trn= MultinomialNB()\n",
        "trained = train(trn)\n",
        "prob_results_sk,prob_results_wo_log_sk = test(trained)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVFWlSv-CYi9"
      },
      "source": [
        "#### Part 2.2: Your Multinomial Naive Bayes Implementation (20 pts = 5 + 5 + 5 + 5)\n",
        "\n",
        "In this part, you are going to implement multinomial naive bayes function by your self, then use your own multinomial naive bayes function to finish the sentimate classification task, and get the probability result using ***predict_proba_without_log*** and ***predict_proba_with_log***.\n",
        "\n",
        "Hint: Similar to Part 1, to get the predicted results, please use <u>***np.argmax(..., axis=1)***</u>, and the ... part should be the probability result obtained from ***predict_proba_without_log*** and ***predict_proba_with_log***.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "prqy9nD8CYi9"
      },
      "outputs": [],
      "source": [
        "# My Multinomial Naive Bayes Function\n",
        "\n",
        "class My_MultinomialNB():\n",
        "    \"\"\"\n",
        "    Multinomial Naive Bayes\n",
        "    ==========  \n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, optional (default=1.0)\n",
        "        Additive (Laplace/Lidstone) smoothing parameter\n",
        "        (0 for no smoothing).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "#         Given feature X and label y, calculate beta and pi with a smoothing parameter alpha (laplace smoothing)\n",
        "        self.class_indicator = {}\n",
        "        for i, c in enumerate(np.unique(y)):\n",
        "            self.class_indicator[c] = i\n",
        "        self.n_class = len(self.class_indicator)\n",
        "        self.n_feats = np.shape(X)[1]\n",
        "        \n",
        "        self.beta    = np.zeros((self.n_class, self.n_feats))\n",
        "        self.pi      = np.zeros((self.n_class))\n",
        "        '''\n",
        "            TODO: \n",
        "            Please calculate self.beta and self.pi\n",
        "        '''\n",
        "\n",
        "        for i in range(self.n_class):\n",
        "          self.pi[i] = np.sum(y==i)/len(y) \n",
        "\n",
        "        for i in range(self.n_class):\n",
        "          self.beta[i,:] = (np.sum(X[(y==i)],0)+1)/(np.sum(np.sum(X[(y==i)],0),0)+self.n_feats)\n",
        "           \n",
        "    \n",
        "    def predict_proba_without_log(self, X):\n",
        "#         Given a test dataset with feature X, calculate the predicted probability of each data point\n",
        "        prob = np.zeros((len(X), self.n_class))\n",
        "        for i in range(self.n_class):\n",
        "          prob[:,i] = np.prod(np.power(self.beta[i,:],X),1)*self.pi[i]\n",
        "\n",
        "        prob = prob/(np.sum(prob,1).reshape(-1,1)) \n",
        "        return prob\n",
        "                       \n",
        "        '''\n",
        "            TODO: \n",
        "            Calculate probability of which class each data belongs to, using self.beta and self.pi\n",
        "        '''\n",
        " \n",
        "    def predict_proba_with_log(self, X):\n",
        "        log_prob = self.predict_log_proba_with_log(X)\n",
        "        log_val = np.exp(log_prob - np.max(log_prob, axis=1).reshape(-1, 1))\n",
        "        log_val = log_val / (np.sum(log_val,1).reshape(-1,1))\n",
        "        return log_val\n",
        "    \n",
        "    def predict_log_proba_with_log(self, X):\n",
        "#         Given a test dataset with feature X, calculate the log probability of each data point\n",
        "        log_prob = np.zeros((len(X), self.n_class))\n",
        "        '''\n",
        "            TODO: \n",
        "            Calculate log-probability of which class each data belongs to, using self.log_beta and self.log_pi\n",
        "        '''\n",
        "        self.log_beta = np.log(self.beta)\n",
        "        self.log_pi = np.log(self.pi)\n",
        "      \n",
        "        log_prob = np.matmul(X,np.transpose(self.log_beta)) + self.log_pi\n",
        "      \n",
        "        return log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CizEvDoVCYi-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3452a090-7974-41a3-c256-e7b00159a415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ],
      "source": [
        "# train and test with My_MultinomialNB\n",
        "\n",
        "\n",
        "'''\n",
        "    TO DO: \n",
        "    Please implement train and test using My_MultinomialNB you implemented above.\n",
        "    You are expected to get the probability result using \"predict_proba_without_log\" and \"predict_proba_with_log\".\n",
        "    For this part, please use the default alpha value: alpha = 1.\n",
        "'''\n",
        "def train(x,y,trn):\n",
        "  x = x\n",
        "  y = y\n",
        "  trn.fit(x,y)\n",
        "  return trn\n",
        "\n",
        "def test(x,trn):\n",
        "\n",
        "  prob_results = trn.predict_proba_with_log(x)\n",
        "  prob_results_wo_log = trn.predict_proba_without_log(x)\n",
        "\n",
        "  return prob_results, prob_results_wo_log\n",
        "\n",
        "trn1= My_MultinomialNB()\n",
        "trained1 = train(X_train_feature,twenty_train[\"target\"],trn1)\n",
        "prob_my, prob_wo_log_my = test(X_test_feature, trained1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A18vnCFjCYi_"
      },
      "source": [
        "#### Part 2.3: Complare sklearn MultinomialNB and your own My_MultinomialNB (15 pts = 2 + 2 + 2 + 2 + 2 + 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt334ZM-CYi_"
      },
      "source": [
        "In part 1 and part 2, you've calculated the probability of test data using ***pred_proba*** and ***pred_log_proba***, and ***predict_proba_without_log*** and ***predict_proba_with_log***. \n",
        "\n",
        "Please answer:\n",
        "1. Compare the accuracy, are they same or not? \n",
        "2. If they are different, please try to explain the reason.\n",
        "\n",
        "__Hint :__\n",
        "\n",
        "The accuracy of sklearn Multinomial NB with log and My_MultinomialNB with log should be larger than $0.9$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WF-4f8klCYi_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a4a50563-adaf-4570-87cd-0d478184aaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn MultinomialNB without log accuracy =  0.9347536617842876\n",
            "My_MultinomialNB without log accuracy =  0.36684420772303594\n",
            "sklearn MultinomialNB with log accuracy =  0.9347536617842876\n",
            "My_MultinomialNB with log =  0.9347536617842876\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    acc = np.equal(y_true, y_pred)\n",
        "    score = sum(acc)/len(acc) # calculate the percentage of the correctness\n",
        "    return score\n",
        "\n",
        "y_true = twenty_test[\"target\"]\n",
        "# accuracy of sklearn MultinomialMB without log\n",
        "'''\n",
        "    TO DO: compute accuracy of sklearn MultinomialNB without log and print it out\n",
        "'''\n",
        "\n",
        "\n",
        "y_pred1 = np.argmax(prob_results_wo_log_sk,1)\n",
        "print(\"sklearn MultinomialNB without log accuracy = \", accuracy(y_true,y_pred1))\n",
        "\n",
        "\n",
        "# accuracy of My_MultinomialMB without log\n",
        "'''\n",
        "    TO DO: compute accuracy of My_MultinomialNB without log and print it out\n",
        "'''\n",
        "\n",
        "\n",
        "y_pred2 = np.argmax(prob_wo_log_my,1)\n",
        "print(\"My_MultinomialNB without log accuracy = \", accuracy(y_true,y_pred2))\n",
        "\n",
        "\n",
        "\n",
        "# accuracy of sklearn MultinomialMB with log\n",
        "'''\n",
        "    TO DO: compute accuracy of sklearn MultinomialNB with log and print it out\n",
        "'''\n",
        "y_pred3 = np.argmax(prob_results_sk,1)\n",
        "print(\"sklearn MultinomialNB with log accuracy = \", accuracy(y_true,y_pred3))\n",
        "\n",
        "# accuracy of My_MultinomialMB with log \n",
        "'''\n",
        "    TO DO: compute accuracy of My_MultinomialNB with log and print it out\n",
        "'''\n",
        "y_pred4 = np.argmax(prob_my,1)\n",
        "print(\"My_MultinomialNB with log = \", accuracy(y_true,y_pred4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1YD6ktsCYjA"
      },
      "source": [
        "#### Write Your answer here:\n",
        "1)So we have printed out 4 accuracies and we observe that the My Multinomial without log gives an accuracy of 36%. Rest all accuracies are same equal to 93.47%. Hence all accuracies are not same.   \n",
        "2) The reason for My Multinmial without log giving so less accuracy is due to the fact that when we dont take log of the probabilty while computing it, we basically take the product of large number of small values (betas are very small). This leads to many of the probabilties being very small and hence when we take argmax, the inbuilt numpy function is built such that when there are multiple max values, it will simply return the first occurence which has highest value. In our case, its simply that all probabilties are almost 0 (they are indeed equal to 0 since this is due to the representation power of this notebook, after some point, like 1e-1000, is simply represented as 0), hence np.argmax will return the first index in this case, which might not be correct always, and hence we observe a low accuracy. This is not the case when we take the log. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR_3cVMlACzA"
      },
      "source": [
        "## Problem 2:  Logistic Regression (50 pts)\n",
        "\n",
        "Consider the Logistic Regression introduced in [lecture 02 - Probabilistic Classifiers](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf) (please refer to page 30-43). In this problem, you are going to understand the derivations of Logistic Regression and apply it on a real-world classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEf7Rc4GrIe"
      },
      "source": [
        "### Part 1: Logistic Regression Derivations (10pts)\n",
        "\n",
        "Write down the matrix form operation for gradient vector and Hessian matrix for logistic regression. You are expected to represent the 1st order and 2nd order derivatives in matrix/vector form, which means you should represent them in the form of multiplication/addition/subtraction of several vectors/matrices. Your representations shouldn’t include $\\sum_i$.\n",
        "\n",
        "\n",
        "Hint: \n",
        "1. Please refer to the derivations on slide 40-41, and turn them into matrix form operations. \n",
        "2. You can define vector p=sigmoid(dot(x, beta)) and matrix P=diag(dot(p,(1-p)))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__lVZsAGrIf"
      },
      "source": [
        "#### Write Your answer here:\n",
        "![pic](https://drive.google.com/uc?export=view&id=18xgTLDmfTslBkFNnhllh1EFUQmYkrcqU)\n",
        "![pic](https://drive.google.com/uc?export=view&id=1mknDiXCrKJPY0NSgmchLK0PrF0EYw6Eg)\n",
        "![pic](https://drive.google.com/uc?export=view&id=1pyEGjFbH7xWxyInyps7Srm6owDGWl9kM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6_F5_EkGrIf"
      },
      "source": [
        "### Part 2: Sklearn Implementation (5 pts)\n",
        "\n",
        "In this part, you are going to work on the classification task on the __breast_cancer__ dataset using the logistic regression function __LogisticRegression__ implemented in the sklearn package. We've provided the data processing parts, please implememt the code for training and testing. Please use variable name **pred_y** for your predicted test results.\n",
        "\n",
        "\n",
        "__Hint__: \n",
        "\n",
        "1. You can refer to https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html to get familiar with the breast_cancer dataset.\n",
        "\n",
        "2. You can refer to https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html to get familiar with sklearn LogisticRegression function.\n",
        "\n",
        "3. You can change the **penalty** and **solver** parameters in the definition of __LogisticRegression__ to observe the effect of different penalties, and choose the one that you think is the best. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J8arc6z0GrIg"
      },
      "outputs": [],
      "source": [
        "# load dataset, split train and test \n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import preprocessing \n",
        "from numpy.linalg import inv\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "n_data, n_features = X.shape[0], X.shape[1]\n",
        "ids  = np.arange(n_data)\n",
        "np.random.seed(1)\n",
        "np.random.shuffle(ids)\n",
        "train_ids, test_ids = ids[: n_data // 2], ids[n_data // 2: ]\n",
        "\n",
        "train_X, train_y = X[train_ids], y[train_ids]\n",
        "test_X,  test_y  = X[test_ids],  y[test_ids]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bFqMeyXLGrIg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "05af813d-2caa-4ebb-94fa-c4a65f5fac2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.96       109\n",
            "           1       0.97      0.98      0.97       176\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.97      0.96      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "# train and test with LogisticRegression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "'''\n",
        "    TO DO: \n",
        "    Please implement train and test using sklearn LogisticRegression.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def train(trn, X,y):\n",
        "  trn2 = trn.fit(X, y)\n",
        "  return trn2\n",
        "def test(trn,X):\n",
        "  pred_y = trn.predict(X)\n",
        "  return pred_y\n",
        "\n",
        "trn = LogisticRegression()\n",
        "trained = train(trn, train_X, train_y)\n",
        "pred_y = test(trained,test_X)\n",
        "\n",
        "# print the evaluation results\n",
        "print(classification_report(test_y,pred_y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2dBTmcbGrIh"
      },
      "source": [
        "### Part 3: Your LogisticRegression Implementation (35pts = 5 + 10 + 10 + 5 + 5)\n",
        "\n",
        "In this part, you are going to implement logistic regression function by your self. \n",
        "\n",
        "For optimizing Logistic Regression, you should implement the matrix version (__matrix_GA_optimizer__) of Gradient Ascent algorithm, and matrix form Newton-Raphson algorithm (__matrix_Newton_optimizer__). Please use 1e-2 as your learning rate.\n",
        "\n",
        "Then, you are going to complete the __fit__ and __predict__ function in ***My_Logistic_Regreesion***.\n",
        "\n",
        "Finally, you are going to get the predicted results using your own ***My_Logistic_Regression*** function with the matrix version optimizer and the Newton-Raphson optimizer.\n",
        "\n",
        "__Hint__: \n",
        "\n",
        "There should not be any for loop inside the __matrix_GA_optimizer__ and __matrix_Newton_optimizer__. \n",
        "\n",
        "There can be for loop inside the __fit__ and __predict__ function.\n",
        "\n",
        "The accuracy of your implementation can be lower than the sklearn version but should be at least close to $0.9$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2t_w5dPsGrIh"
      },
      "outputs": [],
      "source": [
        "# Sigmoid function\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "      TO DO:\n",
        "      Please implement the sigmoid function\n",
        "    \"\"\"\n",
        "    z = np.clip(z,-100,100)\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "#  Two different Optimizers\n",
        "\n",
        "\n",
        "def matrix_GA_optimizer(W, X, y):\n",
        "#     Gradient Ascent (GA) optimizer implemented with matrix operations.\n",
        "    '''\n",
        "        TO DO: \n",
        "        Please implement the matrix version gradient ascent optimizer\n",
        "        Please use 1e-2 as your learning rate.\n",
        "    '''\n",
        "    lr = 1e-2\n",
        "\n",
        "    dot = np.matmul(X,np.expand_dims(W,1))\n",
        " \n",
        "\n",
        "    p = sigmoid(dot)\n",
        "    y = np.expand_dims(y,1)\n",
        "    grad = np.matmul(np.transpose(X),(y-p))\n",
        "    grad = np.squeeze(grad)\n",
        "    W = W + lr*grad\n",
        "    return W\n",
        "\n",
        "def matrix_Newton_optimizer(W, X, y):\n",
        "#     Newton's method optimizer implemented with matrix operations.\n",
        "    '''\n",
        "        TO DO:\n",
        "        Please implement the matrix version newton-raphson optimizer\n",
        "    '''\n",
        "\n",
        "    dot = np.matmul(X,np.expand_dims(W,1))\n",
        "   \n",
        "    p = sigmoid(dot)\n",
        "    y = np.expand_dims(y,1)\n",
        "    grad = np.matmul(np.transpose(X),(y-p))\n",
        "    grad = grad.squeeze()\n",
        "    p = np.squeeze(p)\n",
        "\n",
        "    mid_matrix = np.diag(p*(1-p))\n",
        "    hess = np.matmul((np.matmul(np.transpose(X),mid_matrix)),X)\n",
        "\n",
        "    W = W + np.matmul(np.linalg.pinv(hess),grad)\n",
        "    return W\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l3l4l1UyGrIi"
      },
      "outputs": [],
      "source": [
        "# Your own Logistic Regression Function\n",
        "\n",
        "class My_Logistic_Regression():\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#         n_features : int, feature dimension\n",
        "#         optimizer  : function, one optimizer that takes input the model weights\n",
        "#                      and training data to perform one iteration of optimization.\n",
        "\n",
        "    def __init__(self, n_features, optimizer):\n",
        "        self.W = np.random.rand(n_features+1)/100\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "          TO DO:\n",
        "          Please implement the fit function for X, y using the optimizer you write\n",
        "        \"\"\"\n",
        "\n",
        "        curr_loss = 1\n",
        "        prev_loss = 0\n",
        "        iterr = 1000\n",
        "        one_array = np.ones((len(X),1))\n",
        "        X = (train_X - np.min(X,0))/(np.max(X,0)-np.min(X,0))\n",
        "        X = np.concatenate((one_array,X),1)\n",
        "        count = 0\n",
        "        while(count<iterr):\n",
        "          count += 1\n",
        "          prev_loss = curr_loss\n",
        "          curr_loss = (sum(y*np.matmul(X,self.W) - np.log(1+np.exp(np.clip(np.matmul(X,self.W),-100,100)))))\n",
        "          self.W = self.optimizer(self.W,X,y)\n",
        "     \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "          TO DO:\n",
        "          Please complete the predict function given your learned W\n",
        "          Specifically, you need to compute the probability given X and W\n",
        "        \"\"\"\n",
        "        one_array = np.ones((len(X),1))\n",
        "        X = (X - np.min(X,0))/(np.max(X,0)-np.min(X,0))\n",
        "        X = np.concatenate((one_array,X),1)\n",
        "        return((sigmoid(np.matmul(X,self.W))>0.5))\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iQKNZ3QJGrIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "048fdccb-a6d4-4f4f-bae5-24e84717abff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 0 s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.92      0.96       109\n",
            "           1       0.95      1.00      0.98       176\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.98      0.96      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "Training time: 1 s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       109\n",
            "           1       0.97      0.97      0.97       176\n",
            "\n",
            "    accuracy                           0.96       285\n",
            "   macro avg       0.96      0.96      0.96       285\n",
            "weighted avg       0.96      0.96      0.96       285\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Finally, run the following code to train and test with your own My_Logistic_Regression function\n",
        "# Compare the accuracy and running time of both optimization versions (for-loop and matrix).\n",
        "for optimizer in [matrix_GA_optimizer, matrix_Newton_optimizer]:\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    my_clf = My_Logistic_Regression(n_features, optimizer)    \n",
        "    my_clf.fit(train_X, train_y)\n",
        "    my_pred_y = my_clf.predict(test_X)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print('Training time: %d s' % (end_time - start_time))\n",
        "    print(classification_report(test_y,my_pred_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Problem: Poisson Regression (10 pts)\n",
        "\n",
        "Consider the Generalized Linear Model (GLM) introduced in [lecture 02 - Probabilistic Classifiers](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf) (please refer to page 46-48)."
      ],
      "metadata": {
        "id": "924BQQajLuka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 Poisson Regression Derivation (2 pts)\n",
        "We know that the Poisson distribution is $P(y, \\lambda)=\\frac{\\lambda^ye^{-\\lambda}}{y!}$. Let $\\eta=\\ln(\\lambda)$. In Poisson regression, we use linear model to predict the value of $\\eta$, i.e., $\\eta=\\mathbf{x}^T\\boldsymbol{\\beta}$. Please write down $P(y;\\mathbf{x},\\boldsymbol{\\beta})$."
      ],
      "metadata": {
        "id": "rN8nUxDYMXtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your answer here:\n",
        "$P(y;x,\\beta)$ = $\\frac{1}{y!}e^{y(\\mathbf{x}^T\\boldsymbol{\\beta})-e^{\\mathbf{x}^T\\boldsymbol{\\beta}}}$.  \n",
        "Here η = $x^{T}β$,  \n",
        "T(y) = y,  \n",
        "b(y) = $\\frac{1}{y!}$,  \n",
        "a(η) = $e^{x^{T}β}$"
      ],
      "metadata": {
        "id": "wYhegj7ZQhIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 Optimization (1 pt)\n",
        "Given a training dataset $D=\\{\\mathbf{x}_i,y_i\\}_{i=1,\\cdots,n}$, please name an algorithm that can be used to learn the parameters $\\boldsymbol{\\beta}$. Explain your answer."
      ],
      "metadata": {
        "id": "bUi2DnHRDxse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your answer here:\n",
        "We can use Linear regression with Gradient ascent optimization to learn the parameters. The function we will maximize is the standard likelihood function and we know that it is concave, hence if we simply set the derivate to 0 with respect to beta, we will aproach its maximum in gradient ascent approach."
      ],
      "metadata": {
        "id": "gyDZ2N1nEDTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3 Application (2 pts)\n",
        "Please write down at least two real-world applications you think Poisson regression can solve. "
      ],
      "metadata": {
        "id": "LRgtr5zNY-1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Your answer here:\n",
        "Poisson Regression is mainly used to model count based data, where the output is usually an integral value ranging in non negative integers.  \n",
        "\n",
        "1) We can model the number of customers who would come at a shop on a particular day.    \n",
        "The features we can take are: the day number (ranging from 1 to 7 for 7 days of week), weather of that day, is the day holiday or not, number of customers coming on previous day etc.  \n",
        "\n",
        "2) Number of people visiting a website per hour.  \n",
        "Again the features can be, which hour of day is it, network load on the website domain, number of people visiting site in last few days, rating of website or product on wesbite (if someone has put a feedback form, we can get this data) etc.\n"
      ],
      "metadata": {
        "id": "Nq4AX9c2ZUp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Inference (5 pts = 2 pts + 3 pts)\n",
        "\n",
        "We want to use Poisson regression to predict the number of storms in 4 cities this winter. $\\mathbf{x}$ is the features of all the cities and $\\boldsymbol{\\beta}$ is already learned. \n",
        "* Please implement the code to predict the __expected number__ of storms for each city ($E(Y|\\mathbf{x},\\beta)$) and output your results. \n",
        "* Please implement the code to compute the probability of $P(Y\\ge5;x,\\beta)$ for each city and output your results.\n",
        "\n",
        "__Hint :__\n",
        "\n",
        "1. For Poisson distribution, $E(Y|\\lambda)=\\lambda$. \n",
        "\n",
        "2. $P(y\\ge5;x,\\beta)=1-P(y=0;x,\\beta)-P(y=1;x,\\beta)-P(y=2;x,\\beta)-P(y=3;x,\\beta)-P(y=4;x,\\beta)$.\n"
      ],
      "metadata": {
        "id": "aeOvOAfaRhPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X=np.array([[0.1,0.3,0.5],[2.7,1.6,0.1],[0.1,2.4,0.3],[0.6,0.7,5.0]])\n",
        "beta=np.array([0.1,0.5,-0.2])\n",
        "\n",
        "\"\"\"\n",
        " TO DO:\n",
        " Please implement the code to compute and print the P(y;x,\\beta) \n",
        "\"\"\"\n",
        "n = np.matmul(X,beta)\n",
        "mean = np.exp(n)\n",
        "print(\"Expected number of storms for each city = \")\n",
        "print(mean)\n",
        "\n",
        "def poisson(mean,k):\n",
        "  return ((mean**k)*np.exp(-mean))/(np.math.factorial(k))\n",
        "suma = 0\n",
        "for i in range(5):\n",
        "  suma += poisson(mean,i)\n",
        "print(\"Probabilty that number of storms greater than equal to 5 in each city = \")\n",
        "print(1-suma)  \n"
      ],
      "metadata": {
        "id": "1-amveJ2SXGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f9f1f95b-6871-456d-83f1-74b4e9157510"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected number of storms for each city = \n",
            "[1.06183655 2.85765112 3.15819291 0.55432728]\n",
            "Probabilty that number of storms greater than equal to 5 in each city = \n",
            "[0.00469862 0.16141173 0.21198157 0.00027568]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "105710622.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}